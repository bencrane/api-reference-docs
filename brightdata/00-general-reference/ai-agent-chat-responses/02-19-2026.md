# Bright Data — LinkedIn Jobs Data & Programmatic Delivery

## 1. Ready-Made / Managed Datasets (Recommended for Bulk Jobs Data)

For large-scale or repeated access to LinkedIn jobs, use the Datasets platform:

### Marketplace / Custom LinkedIn Jobs Datasets (via Datasets Platform)

You can:

- Discover jobs by criteria (e.g., location, keyword, country, time range, job type) using the Datasets APIs / SDK.
- Collect specific job postings by URL.

### Typical Job Fields

Exact schema depends on the specific dataset / configuration.

**Job Details:**

- `job_posting_id`
- `job_title`
- `job_summary`
- `job_seniority_level`
- `job_function`
- `job_employment_type`
- `job_industries`
- `job_base_pay_range`
- `base_salary`
- `job_description_formatted` (full description)
- Additional metadata fields depending on configuration

**Company Details:**

- `company_name`
- `company_id`
- `company_url`
- `company_logo`

**Job Metadata:**

- `job_location`
- `job_posted_time`
- `job_posted_date`
- `job_num_applicants`
- `apply_link`
- `country_code`
- `title_id`

**Job Poster Info:**

- `job_poster` (where available)

### Working with LinkedIn Datasets from the UI

Start from the LinkedIn company dataset (example Marketplace entry): LinkedIn company information dataset

From there, you can:

- Open the dataset page.
- Use "Preview sample" to see the exact fields.
- Use filters or request a Custom Dataset if you specifically need job postings (e.g., by role, geography, seniority, etc.).

If you prefer to integrate via code, the JS SDK supports LinkedIn datasets, including job discovery and collection (e.g. `discoverJobs`, `collectJobs`), as shown in the docs.

---

## 2. Real-Time LinkedIn Jobs Web Scraper API

If you want to scrape jobs directly from LinkedIn in real time, use the LinkedIn Jobs Web Scraper API.

**Product page & docs:**

- LinkedIn Jobs Web Scraper API
- LinkedIn Jobs API docs

You can:

- Collect by job URL (single posting).
- Discover jobs by search URL or keywords + filters (location, time range, job type, etc.).

The output fields are similar to the list above (job details, company details, metadata, poster info). The full field list is in the "Output reference" section of the Jobs API page.

---

## 3. Other LinkedIn Data You Might Combine with Jobs

Depending on your use case, you might also want:

- **LinkedIn company data** (for richer firmographics): LinkedIn company dataset / LinkedIn Companies API docs
- **LinkedIn people / profiles or posts** datasets & APIs (for talent or engagement analysis): LinkedIn APIs overview

---

## 4. Recommended Starting Points

**For a pre-built solution:**

- Open the LinkedIn dataset page: LinkedIn company information dataset
- Use it as an entry point to explore LinkedIn datasets and request a Custom Dataset specifically for job postings if needed.

**For a web scraper:**

- Use the LinkedIn Jobs Web Scraper: LinkedIn Jobs Web Scraper

**For building your own flows in code:**

- Use the Scraping IDE / automation: Scraping IDE

---

## 5. Programmatic Delivery Options

### Dataset Products (Marketplace / Custom Datasets / LinkedIn Datasets)

**File Formats:**

- JSON
- NDJSON / JSON Lines
- CSV
- (for some flows) Parquet, XLSX

### Delivery Methods (Programmatic-Friendly)

**API Download**

Use the Dataset API to:

- Request/Initiate a collection
- Get a `snapshot_id`
- Download the file via API

Docs:

- Marketplace Dataset API overview
- Deliver snapshot (data delivery & export)

**Webhook**

- Bright Data sends an HTTP POST to your endpoint when data is ready (or in streaming batches if enabled).
- You parse the payload and load it into your DB or pipeline.

**Cloud Storage (most common for data engineering)**

- Amazon S3
- Google Cloud Storage
- Microsoft Azure Storage
- Google Cloud Pub/Sub
- Snowflake
- SFTP/FTP

Docs: Delivery options (Datasets & Scrapers)

**Email**

- Not really "programmatic," but available (usually for manual download).

### Typical Integration Pattern into Your DB

1. Configure delivery to S3 / GCS / Azure / Snowflake / SFTP in "Delivery settings" of the dataset.
2. Bright Data writes files like `snapshot_XXXX.jsonl` or `.csv`.
3. Your ETL/ELT (Airflow, dbt, custom scripts, etc.) ingests from that storage into your database or warehouse.

---

## 6. Web Scraper API & Scraping IDE Delivery

**Delivery Options:**

- Direct API response (for real-time / small jobs)
- Webhook
- Cloud storage: S3, GCS, Azure, Alibaba OSS
- SFTP/FTP
- API download (similar to datasets)
- Email

**Docs:**

- Web Scraper / Crawl API quick start: Crawl API quick start
- IDE delivery options: Initiate collection & delivery options

Typical pattern: deliver to storage (S3/GCS/Azure/Snowflake/SFTP), then load into your DB with your own pipeline.

---

## 7. Direct Database Delivery?

Bright Data does **not** write directly into arbitrary SQL databases (e.g., Postgres/MySQL) as a built-in option. Instead, you use:

- **Snowflake integration** (if you use Snowflake), or
- **Cloud storage / SFTP / API download** + your own loader into your DB.

---

## 8. Key Doc Links

- Dataset Marketplace overview (incl. delivery & export): Dataset Marketplace overview
- Dataset delivery options (formats + S3/GCS/Azure/Snowflake/SFTP/Webhook): Delivery options
- Marketplace Dataset API (request, status, download, deliver): Marketplace Dataset API overview / Deliver snapshot

---

## LinkedIn Jobs — Easiest Programmatic Setup

1. Use a LinkedIn dataset / LinkedIn Jobs collection
2. Deliver to S3 or GCS in NDJSON
3. Have your pipeline load into your DB
# Bright Data — Indeed Jobs Data & Programmatic Delivery

## 1. Pre-Built Indeed Jobs Dataset (Marketplace)

Ready-made Indeed Job Listings Information dataset: Indeed job listings information dataset

You can:

- Use it as-is (US/UK jobs, with fields like title, company, description, location, salary, publish date, rating, etc.).
- Or request a Custom Dataset if you need specific geos, roles, or fields.

### Programmatic Delivery Options

Same Dataset delivery stack as LinkedIn:

**Formats:**

- JSON
- NDJSON / JSON Lines
- CSV
- (sometimes Parquet, depending on configuration)

**Delivery Methods:**

- API download (Dataset API with `snapshot_id`)
- Webhook
- Cloud storage:
  - Amazon S3
  - Google Cloud Storage
  - Microsoft Azure Storage
  - Google Cloud Pub/Sub
  - Snowflake
  - SFTP/FTP
- Email (for manual use)

**Docs:**

- Marketplace overview & delivery: Dataset Marketplace overview
- Delivery options
- Deliver snapshot via API

### Typical Integration Pattern

1. Configure the Indeed dataset's Delivery settings to S3/GCS/Azure/Snowflake/SFTP.
2. Bright Data writes files like `indeed_jobs_YYYYMMDD.jsonl`.
3. Your ETL/ELT loads those into your DB / warehouse.

---

## 2. Indeed Web Scraper (For Fresh / Custom Jobs)

If you need live or highly customized Indeed jobs (e.g., specific queries, schedules), use the Indeed Web Scraper: Indeed web scraper

**Delivery options (same as other scrapers):**

- Direct API response (for real-time calls)
- Webhook
- Cloud storage: S3, GCS, Azure, Alibaba OSS
- SFTP/FTP
- API download
- Email

**Docs (generic scraper delivery):** Initiate collection & delivery options

Typical pattern: deliver to S3/GCS/Azure/Snowflake/SFTP, then load into your DB with your own pipeline.

---

## 3. Summary for Indeed Jobs

**Via Dataset (pre-built or custom):**

- API download, Webhook, S3/GCS/Azure/Snowflake/SFTP, etc.

**Via Web Scraper:**

- Same delivery options, plus real-time API responses.

There is no direct "write into my Postgres/MySQL" option — you use storage / Snowflake / API + your own loader.